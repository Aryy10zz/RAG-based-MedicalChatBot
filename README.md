# RAG-based-MedicalChatBot
Project Overview

This project implements a Retrieval-Augmented Generation (RAG) based Medical Chatbot designed to provide accurate, contextual, and reliable medical information related to diseases, symptoms, precautions, and remedies.

Instead of relying solely on a Large Language Model (LLM), the chatbot retrieves relevant medical knowledge from a curated Hugging Face disease dataset, ensuring responses are fact-grounded and domain-specific. The retrieved context is then passed to Gemini Flash LLM to generate precise and user-friendly answers.

The system achieves 90%+ response accuracy, making it suitable for educational and informational medical use cases.

ğŸš€ Key Features

ğŸ§  Retrieval-Augmented Generation (RAG) architecture for factual accuracy

ğŸ“š Medical knowledge ingestion from Hugging Face Disease Database

ğŸ” Vector search using ChromaDB for fast and relevant retrieval

ğŸ¤– Gemini Flash LLM for low-latency, high-quality responses

ğŸ“ˆ Accurate answers on:

Diseases

Symptoms

Precautions

Remedies

ğŸ““ Implemented as an interactive Jupyter Notebook

ğŸ—ï¸ System Architecture (RAG Pipeline)
User Query
   â†“
Query Embedding
   â†“
ChromaDB Vector Search
   â†“
Relevant Medical Context Retrieved
   â†“
Context + Query â†’ Gemini Flash LLM
   â†“
Final Medical Response

ğŸ§‘â€âš•ï¸ How It Works

Data Ingestion
Medical data from a Hugging Face disease dataset is loaded and preprocessed.

Indexing with LlamaIndex
LlamaIndex structures the medical documents and prepares them for efficient retrieval.

Vector Storage
Document embeddings are stored in ChromaDB, enabling semantic similarity search.

Query Processing

User queries are embedded

Relevant medical documents are retrieved from ChromaDB

Response Generation
Retrieved context is passed to Gemini Flash LLM, which generates accurate and contextual responses.

ğŸ› ï¸ Tech Stack

Python

LlamaIndex â€“ document indexing and retrieval

ChromaDB â€“ vector database

Hugging Face Datasets â€“ medical knowledge base

Gemini Flash LLM â€“ response generation

Jupyter Notebook

ğŸ“‚ Project Structure
RAG-based-MedicalChatBot/
â”‚
â”œâ”€â”€ llamaindex_medical_retrieval.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt (optional)

â–¶ï¸ How to Run the Project
1ï¸âƒ£ Clone the Repository
git clone https://github.com/Aryy10zz/RAG-based-MedicalChatBot.git
cd RAG-based-MedicalChatBot

2ï¸âƒ£ Install Dependencies
pip install llama-index chromadb datasets google-generativeai

3ï¸âƒ£ Run the Notebook

Open llamaindex_medical_retrieval.ipynb in Jupyter Notebook or VS Code and execute the cells sequentially.

ğŸ“Š Results

âœ”ï¸ 90%+ accurate responses

âœ”ï¸ Reduced hallucinations using retrieval-based grounding

âœ”ï¸ Low latency due to Gemini Flash integration

âœ”ï¸ High relevance for medical queries

âš ï¸ Disclaimer

This chatbot is intended only for educational and informational purposes.
It does not provide medical diagnosis or professional medical advice.
Always consult a qualified healthcare professional for medical concerns.

ğŸ§© Portfolio Highlights

Demonstrates real-world RAG implementation

Combines LLMs with vector databases

Strong example of applied GenAI in healthcare

Focus on accuracy, reliability, and system design

ğŸ™Œ Acknowledgements

Hugging Face for medical datasets

LlamaIndex for RAG framework

Google Gemini Flash for LLM capabilities

ğŸ‘¤ Author

Aryan Tawde
ğŸ“Œ Aspiring Data Analyst & GenAI Enthusiast
ğŸ”— GitHub: Aryy10zz
